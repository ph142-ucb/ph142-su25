---
title: "L15:  Statistical Inference"
---

### The standard Normal distribution
Recall the Standard Normal

- The standard Normal distribution N(0,1) has $\mu = 0$ and $\sigma=1$.
- $X \sim N(0,1)$, implies that the random variable X is Normally distributed.

```{r, echo=F, out.width="75%"}
#students you do NOT need to know this code
library(ggplot2)
p1 <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), col = "orange") +
  labs(y = "density", x = "") + 
  geom_text(aes(x = 0, y = 0.45), label = "mean = 0 \nsd = 1", check_overlap = T, col = "orange") +
  theme_minimal(base_size = 15) +
  geom_segment(aes(x = 0 , y = 0, xend = 0, yend = 0.4)) +
  geom_segment(aes(x = 1 , y = 0, xend = 1, yend = 0.25), lty = 2) +
  geom_segment(aes(x = -1 , y = 0, xend = -1, yend = 0.25), lty = 2) +
  geom_segment(aes(x = 2 , y = 0, xend = 2, yend = 0.05), lty = 3) +
  geom_segment(aes(x = -2 , y = 0, xend = -2, yend = 0.05), lty = 3) +
  scale_x_continuous(breaks = -3:3, 
                     labels = c(-3, "-2\n-2 SD", "-1\n-1 SD", "0\nmean", "1\n+1 SD", "2\n+2 SD", 3))
p1
```

### Standardizing Normally distributed data

- Any random variable that follows a Normal distribution can be standardized
- If $x$ is an observation from a distribution that has a mean $\mu$ and a 
standard deviation $\sigma$, 

$$z = \frac{x-\mu}{\sigma}$$
   
### What's the Z 

By converting our variable of interest $X$ to $Z$ we can use the probabilities of the standard normal probability distribution to estimate the probabilities associated with $X$.

- A standardized value is often called a **z-score**
- Interpretation: $z$ is the number of standard deviations that $x$ is above or
below the mean of the data.

## Confidence intervals for the mean $\mu$

### Confidence intervals for the mean $\mu$

| Confidence level C | 90%   | 95%                 | 99%   |
|--------------------|-------|---------------------|-------|
| Critical value z*  | 1.645 | 1.960 ($\approx 2$) | 2.576 |

- These numbers correspond to the value on the x-axis corresponding to having 90%,
95%, or 99% of the area under the Normal density between -z and z.

The generic format of a confidence interval is then:
$$\bar{x} \pm z* \frac{\sigma}{\sqrt{n}}$$



### Confidence intervals for the mean $\mu$
- For example, the middle 90% of the area under the Normal density lies between 
-1.645 and 1.645. 

- Thus, a 90% confidence interval is of the form:

$$\bar{x} \pm 1.645\frac{\sigma}{\sqrt{n}}$$

### Confidence interval for the mean of a Normal population

Draw a SRS of size $n$ from a Normal population having unknown mean $\mu$ and
known standard deviation $\sigma$. A level C confidence interval for $\mu$ is:

$$\bar{x} \pm z^*\frac{\sigma}{\sqrt{n}}$$

$$\text{unbiased estimate} \pm \text{(critical value)}\times{\text{(sd of the distribution of the estimate)}}$$

$$\text{unbiased estimate} \pm \text{(critical value)}\times{\text{(standard error)}}$$

### Steps in finding confidence intervals

1. Problem: Statement of the problem in terms of the parameter you would like 
to estimate

2. Plan: How will you estimate this parameter? What type of data will you collect?

3. Data: After you plan the study, collect the data you need to answer the problem.

4. Analysis: Evaluate whether the assumptions required to compute a confidence 
interval are satisfied. Calculate the estimate of the mean and its confidence 
interval. 

5. Conclusion: Return to the practical question to describe your results in this
setting.

### Example on IQ scores (pg. 354)

We are interested in the mean IQ scores of 7th grade girls in a Midwest school 
district. Here are the scores for 31 randomly selected seventh-grade girls. We
also know that the standard deviation of IQ scores is 15 points:

```{r the-sample-data}
scores <- c(114, 100, 104, 89, 102, 91, 114, 114, 103, 105, 
            108, 130, 120, 132, 111, 128, 118, 119, 86, 72,
            111, 103, 74, 112, 107, 103, 98, 96, 112, 112, 93)

iq_data <- data.frame(scores)

known_sigma <- 15
```

Estimate the mean IQ score $\mu$ for all seventh grade girls in this Midwest 
school district by giving a 95% confidence interval.

### Example on IQ scores (pg. 354)

First check the three assumptions:

1. Normality: Can evaluate this using a histogram
2. SRS: Can only use information provided in the problem to assess with an SRS
was taken. 
3. Known $\sigma$: Can use information provided in the problem to determine if
$\sigma$ is known.

### Checking Normality

```{r, warning=F, message=F, echo=F, out.width="65%", fig.align='center'}
library(tidyverse)
ggplot(iq_data, aes(x = scores)) + 
  geom_histogram(binwidth = 5, col = "white") +
  theme_minimal(base_size = 15)
```

We can't examine the Normality of the population (because we don't have data on 
everyone) but we can make a plot for the sample. These data appear slightly left-
skewed, but since there is not much data, it may actually follow a Normal distribution.

### Calculating the estimated mean and its confidence interval

Option 2: Perform calculations using R

```{r calc-confidence-interval}
sample_mean <- mean(scores)

standard_error <- known_sigma/sqrt(length(scores))
critical_value <- 1.96

lower_bound <- sample_mean - critical_value*standard_error
upper_bound <- sample_mean + critical_value*standard_error
```

### Calculating the estimated mean and its confidence interval
```{r con_int2}
sample_mean
standard_error
lower_bound
upper_bound
```


### Calculating the estimated mean and its confidence interval
Thus, our best estimate of the population mean is 105.84. 

Its 95% confidence interval is 100.56 to 111.12. 

If our model assumptions are correct and there is only random error affecting the estimate, this method for calculating confidence intervals will contain the true value $\mu$ 95% of the time (19 times out of 20).

### Recap

* We learned how to create a confidence interval for the mean when the standard deviation for the population is known.
* We learned about the three required assumptions and how to check the Normality assumption using a histogram.
* We learned how to interpret the confidence interval and the definitions for the confidence level and the margin of error
* We introduced our "recipe" for a margin of error

$$\text{unbiased estimate} \pm \text{(critical value)}\times{\text{(standard error)}}$$


### Hypothesis testing

## Example: one tailed hypothesis

### Inorganic phosphorus levels in the elderly

Levels of inorganic phosphorus in the blood are known to vary among adults 
Normally with $\mu = 1.2$ millimoles per liter and standard deviation 0.1 mmol/l.

A study examined inorganic phosphorus in older individuals to see if it decreases
with age. Here are the data from 12 men and women between the ages of 75 and 79:

```{r the-known-info-and-data}
phos <- c(1.26, 1.39, 1, 1, 1, 1, 
          0.87, 1.23, 1.19, 1.29, 1.03, 1.18)

known_sigma <- 0.1
```

*note that sigma here is given to you

### Descriptives

The sample mean $\bar{x}$ is:
```{r, fig.align='center', out.width="80%", echo=F, warning=F, message=F}
library(tidyverse)
mean(phos)
phos_data <- data.frame(phos)
```

Does the sample mean really differ from the null hypothesis mean?

### The null hypothesis $H_0$ and alternative hypothesis $H_a$

The null hypothesis is a hypothesis of no effect or no difference. For our 
example: $$H_0: \mu = 1.2$$


The alternative hypothesis ($H_a$) is the competing hypothesis. In this question, 
the competing hypothesis is that older individuals have an average phosphorus level
lower than that seen in the overall adult population: $$H_a: \mu < 1.2$$

This is a **one-sided hypothesis**

### Descriptives

We can draw the data, the sample mean, and the hypothesized mean under $H_0$ all
on one figure: 

```{r, fig.align='center', out.width="70%", echo=F}
ggplot(phos_data, aes(x = phos)) + 
  geom_histogram(binwidth = 0.05, fill = "forest green", col = "white") + 
  theme_minimal(base_size = 15) +
  geom_vline(xintercept = mean(phos), col = "forest green") +
  geom_text(x = mean(phos)-0.03, y = 4, label = "Sample\nmean", check_overlap = T, col = "forest green") +
  geom_vline(xintercept = 1.2, col = "blue") + 
  geom_text(x = 1.26, y = 4, label = "Null hypothesis\nmean", check_overlap = T, col = "blue") +
  labs(main = "Is the true underlying equal to 1.2?", x = "Phosphorus", y = "Count")
```



### Draw the sampling distribution under the null hypothesis:

What is the probability of observing the sample mean (or lower) we observed if the null 
hypothesis were true?

```{r, fig.align='center', out.width="75%", echo=F}
ggplot(data = data.frame(x = c(1.0, 1.4)), aes(x)) + 
    geom_area(stat = "function", fun = dnorm, args = list(mean = 1.2, sd = 0.0289), 
            fill = "#ec7014", xlim = c(1.1, 1.12)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 1.2, sd = 0.0289)) +
  labs(y = "density") + 
  theme_minimal(base_size = 15) +
  geom_vline(xintercept = 1.2, lty = 1) + 
  geom_vline(xintercept = 1.12, lty = 2) +
  geom_text(x = 1.1, y = 12, label = "observed\nsample\nmean", check_overlap = T) + 
  labs(title = "Sample mean's distribution under the null hypothesis")
```

### Draw the sampling distribution under the null hypothesis:

What is the probability of observing the sample mean we observed *or lower* if 
the null hypothesis were true?

```{r}
pnorm(q = 1.12, mean = 1.2, sd = 0.0289)
```

Thus, under the null hypothesis, this is a 0.28% chance of observing a sample
mean at least as small as what we saw. This is a very tiny probability, and 
suggests that the null hypothesis may not be true and that there is evidence in
favor of the alternative hypothesis.

This probability is known as the **p-value**.

### Example, extended.

Suppose instead that for this example, we chose a sample such that $\bar{x} = 1.17$.

- What is the null and alternative hypotheses?
    - $H_0$:
    - $H_a$:
    
    
- Let's look at the distribution under the null hypothesis and add a line at $\bar{x}$. 


### Example, extended.
```{r, fig.align='center', out.width="75%", echo=F}
ggplot(data = data.frame(x = c(1.0, 1.4)), aes(x)) + 
    geom_area(stat = "function", fun = dnorm, args = list(mean = 1.2, sd = 0.0289), 
            fill = "#ec7014", xlim = c(1.1, 1.17)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 1.2, sd = 0.0289)) +
  labs(y = "density") + 
  theme_minimal(base_size = 15) +
  geom_vline(xintercept = 1.2, lty = 1) + 
  geom_vline(xintercept = 1.17, lty = 2) +
  geom_text(x = 1.1, y = 12, label = "observed\nsample\nmean", check_overlap = T) + 
  labs(title = "Sample mean's distribution under the null hypothesis")
```


### Example, extended.
- Calculate the probability of observing an observation of 1.17 or lower.

```{r}
pnorm(q = 1.17, mean = 1.2, sd = 0.0289)
```



## Two sided testing

### Hypothesis testing with a two-sided hypothesis

Suppose you are interested in the Aspirin content in a sample of pills. You have
been told that the population of Aspirin tablets is Normally distributed and 
has a **known standard deviation of 5 mg**. Furthermore, you have a SRS of $n=10$ pills.
You need to be able to detect if there is evidence that the average Aspirin content
in your sample is different from the population average and would be concerned if
it appears to be higher or lower. Here:

$$H_0: \mu = 325 mg$$

$$H_a: \mu \neq 325mg$$
This is a two-sided alternative hypothesis because we're interested in knowing if
the sample mean appears to be either higher or lower.

### Hypothesis testing with a two-sided hypothesis

1. Check the conditions.
2. Calculate the sampling distribution, assuming the null hypothesis is true:
    - $\mu= 325$
    - $sd(\bar{x})=\sigma / \sqrt{n}=5/ \sqrt{10}=1.581139$
3. Calculate $\bar{x}$ of provided data. In our example here the sample mean is 326.9.
4. Sketch the sampling distribution and add a vertical line at $\bar{x}$. Shade the region corresponding to $H_a$. If the hypothesis is two-sided, add another vertical line that is the same distance as $\bar{x}$ is from $\mu$ but on the other side of the distribution. 

### Two-sided alternative: add vertical lines at $\bar{x}$ and the equivalent distance from the null on the other side of the distribution

```{r, fig.align='center', out.width="80%", echo=F}
#students, you don't need to worry about this code
ggplot(data = data.frame(x = c(320, 330)), aes(x)) + 
  stat_function(fun = dnorm, n = 101, args = list(mean = 325, sd = 1.581139)) +
  labs(y = "density", title = "Two-sided hypothesis test has two shaded regions") + 
  theme_minimal(base_size = 15) +
  geom_vline(xintercept = 325, lty = 1) + 
  geom_vline(xintercept = 326.9, lty = 2) +
  geom_vline(xintercept = 325 - (326.9-325), lty = 2) +
  geom_text(x = 326.9, y = 12, label = "sample\nmean", check_overlap = T) + 
  geom_text(x = 325, y = 12, label = "null\nhypothesis", check_overlap = T) +
  geom_area(stat = "function", fun = dnorm, args = list(mean = 325, sd = 1.581139), 
            fill = "#ec7014", xlim = c(320, 325 - (326.9-325))) + 
  geom_area(stat = "function", fun = dnorm, args = list(mean = 325, sd = 1.581139), 
            fill = "#ec7014", xlim = c(326.9, 330))
```

### Calculation for the hypothesis testing with a two-sided hypothesis

```{r}
pnorm(q = 326.9, mean = 325, sd = 1.581139, lower.tail = F)
pnorm(q = 326.9, mean = 325, sd = 1.581139, lower.tail = F)*2
```

- Why do we need to multiply the probability by 2? 
- Why do we need to set `lower.tail=F`?

Interpret this probability. Does it provide evidence against the null hypothesis
or, in the contrary, does this observation seem to follow under the null 
hypothesis?

## Hypothesis testing using Z tests

### Do you remember z-scores?

We briefly considered z-scores of the form:

$$z=\frac{x-\mu}{\sigma}$$

The z-score tells you how far $x$ is from $\mu$ in terms of units of standard deviation.

Originally, we considered z-scores to examine how far a baby's birthweight deviated from the average birthweight we expected at a given gestational age.

You could make a statement like: This birthweight is 2.5 standard deviations below the mean, which is very small. 

### Do you remember z-scores?

We can generalize the z-score formula to look specifically at z-scores for 
$\bar{x}$, and use its $\mu$ and the standard deviation of the sampling distribution:

$$z = \frac{\bar{x}-\mu}{\sigma/\sqrt{n}}$$

Recall that after standardization of a Normal variable, z~N(0,1)

### Go back to the earlier example looking at phosphorus

$\bar{x}=1.12$

$\mu = 1.2$

$\sigma/\sqrt{n} = 0.0289$     Thus:

$$z = \frac{1.12-1.2}{0.0289}=-2.768166$$

That is, z is 2.77 standard errors below the mean. 

### Go back to the earlier example looking at phosphorus

What is the probability of observing a z-score of -2.768166 (or lower) on the 
standard Normal distribution?

```{r, echo=TRUE}
pnorm(q = -2.768166, mean = 0, sd = 1)
```

The calculation is the same as before. We just standardized the units!

### Go back to the earlier example looking at phosphorus
And we can see that the graph looks the same as well

```{r, fig.align='center', out.width="80%", echo=F}
ggplot(data = data.frame(x = c(-3.5, 3.5)), aes(x)) + 
    geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1), 
            fill = "#ec7014", xlim = c(-3.5, -2.768166)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  labs(y = "density") + 
  theme_minimal(base_size = 15) +
  geom_vline(xintercept = -2.768166, lty = 1) + 
  geom_vline(xintercept = 0, lty = 2) +
  geom_text(x = -2.3, y = 0.35, label = "observed\nsample\nmean", check_overlap = T) + 
  labs(title = "Sample mean's distribution under the null hypothesis")
```


## Definitions

### Test statistic

The Z value we calculated in our example is a kind of test statistic.


**Test Statistic:** Measures how far the data diverge from the null hypothesis 
$H_0$. Large values of the statistic show that the data are far from what we 
would expect if $H_0$ were true.


The Z is drawn from comparisons to a standard normal - we will learn about other tests statistics in part III

### Formalizing what we meant by "test statistic" 

The Z test for a population mean: Draw a SRS from a Normal($\mu$, $\sigma$) 
population, where $\mu$ is unknown, but $\sigma$ is known. A test statistic and 
a p-value are obtained to test the null hypothesis that $\mu$ has a specified value:

$H_0$: $$\mu=\mu_0$$

The one-sample z test statistic is:

$$z = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}$$

### Z test for a population mean (continued)

As the variable $Z$ follows the standard Normal distribution (i.e., N(0,1)), the 
p-value for a test of $H_0$ against

$H_a$: $\mu > \mu_0$ is $P(Z \geq z)$

```{r, out.width="50%", echo=F, fig.align='center'}
upper <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) + 
  geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1),
            xlim = c(1.7, 3), fill = "orange") +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  theme_minimal(base_size = 15) + labs(title = "One-sided (above)") +
  theme(aspect.ratio=1)
upper
```

### Z test for a population mean (continued)

As the variable $Z$ follows the standard Normal distribution (i.e., N(0,1)), the 
p-value for a test of $H_0$ against

$H_a$: $\mu < \mu_0$ is $P(Z \leq z)$

```{r, out.width="50%", echo=F, fig.align='center'}
lower <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) + 
  geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1),
            xlim = c(-3, -1.7), fill = "orange") +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  theme_minimal(base_size = 15) + labs(title = "One-sided (below)") +
  theme(aspect.ratio=1)
lower
```

### Z test for a population mean (continued)

As the variable $Z$ follows the standard Normal distribution (i.e., N(0,1)), the 
p-value for a test of $H_0$ against

$H_a$: $\mu \neq \mu_0$ is $2\times P(Z \geq |z|)$

```{r, out.width="50%", echo=F, fig.align='center'}
both <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) + 
  geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1),
            xlim = c(1.7, 3), fill = "orange") +
  geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1),
            xlim = c(-3, -1.7), fill = "orange") +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  theme_minimal(base_size = 15) + labs(title = "Two-sided") +
  theme(aspect.ratio=1)

#library(patchwork)
both
#upper + lower + both + plot_layout() 
```


